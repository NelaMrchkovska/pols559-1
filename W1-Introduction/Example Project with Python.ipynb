{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will be exploring how President Trump tweets. What does he tweet about? Which tweets lead to high engagement? What about his language style?\n",
    "\n",
    "Note: if a python package can't be found, you can usually install by opening the 'Anaconda prompt' on your desktop and typing 'conda install -c conda-forge packagename' or 'pip install -U packagename' (or search for the package and look for installation instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to retrieve the twitter data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(\"C:/Users/John/Desktop/pols559-master/pols559-master/\")\n",
    "os.getcwd()\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use trumptwitterarchive.com which contains all of the tweets from his personal twitter account since 2009\n",
    "#We will scrape .json formatted files for each tweet by year\n",
    "\n",
    "def get_tweets(year):\n",
    "    url = ('http://www.trumptwitterarchive.com/data/realdonaldtrump/%s.json' %year)\n",
    "    r = requests.get(url)\n",
    "    print(str(year) + ' ' + 'finished.')\n",
    "    return r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tweets(years=range(2009,2020)):\n",
    "    tweets = []\n",
    "    \n",
    "    for year in years:\n",
    "        data = get_tweets(year)\n",
    "        data_length = len(data)\n",
    "            \n",
    "        i = 0\n",
    "        while i < data_length:\n",
    "            tweets.append(data[i])\n",
    "            i = i + 1 \n",
    "                \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"trump_tweets.json\", \"w\") as outfile:\n",
    "    json.dump(save_tweets(), outfile)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the data frame\n",
    "df = pd.read_json(\"trump_tweets.json\")\n",
    "df.shape \n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tk = TweetTokenizer()\n",
    "\n",
    "def tokenize_tweets(data):\n",
    "    text = ''\n",
    "    for token in df['text']:\n",
    "        text = text + token\n",
    "    tokens = [i.lower() for i in tk.tokenize(text)]\n",
    "    return tokens\n",
    "\n",
    "tokens = tokenize_tweets(df)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words, special characters and links (you may or may not want to do this)\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import string\n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_tokens(tokens):\n",
    "    tokens_complete = [t for t in tokens \n",
    "                       if (len(t) >= 3) \n",
    "                       and (not t.startswith(('#', '@'))) # omit hashtag and username\n",
    "                       and (not t.startswith('http')) # omit links \n",
    "                       and (t not in stop) # omit stop words\n",
    "                       and (t[0].isalpha())] # make sure all characters are alphabetical\n",
    "    \n",
    "    return tokens_complete\n",
    "\n",
    "tokens_clean = complete_tokens(tokens)\n",
    "print(len(tokens_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does Trump tweet about?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "hashtags = [t for t in tokens if (t.startswith('#') and len(t) != 1)]\n",
    "hashtags_freq = FreqDist(hashtags).most_common(10)\n",
    "print(hashtags_freq)\n",
    "# it looks like the topics are mostly about promoting his campaign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most commonly mentioned twitter users?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_freq = FreqDist([t for t in tokens if t.startswith(\"@\")]).most_common(10)\n",
    "print(users_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which tweets inspired the most likes and retweets?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the most-liked and most-retweeted tweet\n",
    "import numpy as np\n",
    "like_max = np.max(df['favorite_count'])\n",
    "retweet_max  = np.max(df['retweet_count'])\n",
    "\n",
    "like = df[df.favorite_count == like_max].index[0]\n",
    "print(\"The most-liked tweet from Trump is: \" + \"\\n\" + df['text'][like] + \"\\n\" + \"The number of likes is \" + str(like_max))\n",
    "\n",
    "retweet = df[df.retweet_count == retweet_max].index[0]\n",
    "print(\"The most-retweeted tweet from Trump is: \" + \"\\n\" + df['text'][retweet] + \"\\n\" + \"The number of retweets is \" + str(retweet_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's visualize the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create time series for data\n",
    "time_like = pd.Series(data=df['favorite_count'].values, index=df['created_at'])\n",
    "time_retweet = pd.Series(data=df['retweet_count'].values, index=df['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline will make your plot outputs appear and be stored within the notebook\n",
    "%matplotlib inline\n",
    "time_like.plot(figsize=(16,4), label=\"Number of Likes\", legend=True) \n",
    "time_retweet.plot(figsize=(16,4), label=\"Number of Retweets\", legend=True).set_xlabel(\"Time of Creation\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does his language style look like? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from matplotlib.pyplot import imread\n",
    "import matplotlib.pyplot as pt\n",
    "\n",
    "import os \n",
    "os.chdir('C:/Users/John/Desktop/pols559-master/pols559-master/')\n",  #this is your directory obviously)
    "# paths can be tricky. \n",
    "\n",
    "# delete the retweets for analysis\n",
    "df_own = df[~df.is_retweet].copy() # make a .copy() so that we don't change the initial data frame \n",
    "tokens_own = complete_tokens(tokenize_tweets(df_own[df_own.created_at == '2019'])) # to run it faster, let's look at just one year\n",
    "\n",
    "# we can do something fancier by adding a background picture (https://publicdomainpictures.net/en/index.php)\n",
    "pic = imread('img/trump.jpg')\n",
    "wc = WordCloud(background_color=\"white\",\n",
    "               stopwords=stop,\n",
    "               mask=pic,\n",
    "               scale=3,\n",
    "               max_words=2000,\n",
    "               max_font_size=100, \n",
    "               random_state=40).generate_from_frequencies(FreqDist(tokens_own))\n",
    "pt.imshow(wc)\n",
    "pt.axis(\"off\")\n",
    "wc.to_file('img/trump_wordcloud.jpg')\n",
    "\n",
    "# This may take a while\n",
    "# Hmmm, his most common words seem pretty positive - \"great\", \"thank\", \"people\",\"america\"..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the textblob package assigns polarity values to individual tweets\n",
    "# https://www.geeksforgeeks.org/twitter-sentiment-analysis-using-python/\n",
    "\n",
    "from textblob import TextBlob # https://textblob.readthedocs.io/en/dev/install.html\n",
    "import re\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    '''\n",
    "    Utility function to clean tweet text by removing links, special characters using simple regex statements.\n",
    "    '''\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "\n",
    "def get_tweet_sentiment(tweet):\n",
    "    ''' \n",
    "    Utility function to classify sentiment of passed tweet using textblob's sentiment method \n",
    "    '''\n",
    "    analysis = TextBlob(clean_tweet(tweet))\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return 'Positive'\n",
    "    elif analysis.sentiment.polarity == 0:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Negative'\n",
    "\n",
    "df['sentiment'] = [get_tweet_sentiment(t) for t in df.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare these type of tweets \n",
    "p_tweets = df[(~df.is_retweet) & (df.sentiment == 'Positive')] \n",
    "neu_tweets = df[(~df.is_retweet) & (df.sentiment == 'Neutral')]                                                                   \n",
    "neg_tweets = df[(~df.is_retweet) & (df.sentiment == 'Negative')]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a closer look at each type of tweets \n",
    "p_tweets.text[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neu_tweets.text[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_tweets.text[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The percentage of positive tweets is {round(len(p_tweets)*100/len(df[(~df.is_retweet)]),2)}%.\")\n",
    "print(f\"The percentage of neutral tweets is {round(len(neu_tweets)*100/len(df[(~df.is_retweet)]),2)}%.\")\n",
    "print(f\"The percentage of negative tweets is {round(len(neg_tweets)*100/len(df[(~df.is_retweet)]),2)}%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
